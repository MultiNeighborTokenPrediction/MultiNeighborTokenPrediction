<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Speech Generation Demo</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f0f0f0;
      padding: 20px;
    }

    .container {
      background-color: white;
      border-radius: 8px;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
      padding: 20px;
      max-width: 1200px;
      margin: 20px auto;
    }

    h1 {
      text-align: center;
      font-size: 32px;
      font-weight: bold;
    }

    h2 {
      text-align: center;
      font-size: 20px;
      font-weight: bold;
    }

    h3 {
      margin-top: 30px;
    }

    ul {
      list-style-type: none;
      padding: 0;
    }

    li {
      font-size: 18px;
      font-weight: bold;
      margin: 10px 0;
    }

    a {
      text-decoration: none;
      color: #007bff;
    }

    p {
      font-size: 16px;
      line-height: 1.5;
      margin: 20px 0;
    }

    .centered-paragraph {
      width: 1100px;
      margin: 0 auto;
      text-align: left;
    }

    .table-container {
      margin: 20px auto;
      max-width: 1200px;
      overflow-x: auto;
      display: block;
    }

    table {
      width: 100%;
      border-collapse: collapse;
      border: 1px solid #ccc;
    }

    th,
    td {
      padding: 8px;
      text-align: center;
      border: 1px solid lightgray;
    }

    th {
      border-bottom: 2px solid black;
      background-color: #f9f9f9;
    }

    .sticky-column {
      position: sticky;
      left: 0;
      background-color: white;
      z-index: 1;
      font-weight: bold;
    }

    .empty-cell {
      display: none;
    }

    audio {
      width: 280px;
    }

    img {
      display: block;
      margin: 20px auto;
    }
  </style>
  <!-- 仅加载 MathJax -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <!-- 1. 标题与摘要 -->
  <div class="container">
    <!-- <h1 class="entry-title"> -->
    <h1 class="entry-title" itemprop="headline" style="width: 80%; margin: 0 auto; text-align: center;">
      Improving Speech Generation via Multi-Neighbor Token Prediction for Neural Codec Language Models
    </h1>
    <br>
    <p class="centered-paragraph">
      <b>Abstract.</b> Inspired by Large Language Models (LLMs), codec-based TTS systems have achieved remarkable
      success in high-fidelity speech generation by directly adopting the standard Cross-Entropy (CE) objective to
      predict sequences of discrete speech tokens from neural audio codecs. However, this practice overlooks the
      geometric structure of codec token spaces: tokens that are close in the codec codebook embedding space often
      correspond to acoustically similar speech realizations and can serve as perceptually plausible alternatives. We
      term this observation the <b>Neighboring Token Phenomenon</b>. The one-hot CE objective is agnostic to such
      structure, as it penalizes all non-target tokens equally, including these close neighbors, creating a mismatch
      between the structured codec latent space and the learning objective. In this paper, we quantitatively analyze the
      neighboring token phenomenon in both reconstruction and generation settings. To address the mismatch, we propose
      <b>Multi-Neighbor Cross-Entropy (MNCE)</b> loss function, which generalizes the one-hot target of CE loss into a
      positive set containing the ground-truth token and its nearest neighbors, encouraging the model to treat neighbor
      predictions as partially correct. As a result, even when the model misses the exact target token, it is more
      likely to predict a close neighbor rather than an unrelated token, reducing perceptually harmful errors in
      synthesized speech. Experiments demonstrate that MNCE is effective and generalizable across diverse codec-based
      TTS systems, improving quality and robustness. Moreover, scaling training to 20,000 hours further validates the
      scalability of our approach.
    </p>

    <h2 id="content">Table of Contents</h2>
    <ul class="centered-paragraph">
      <li><a href="#Part 0">Preliminaries: Overall Pipeline of Codec-based TTS and MNCE Loss</a></li>
      <li><a href="#Part 1">1. Analysis of Neighboring Token Phenomenon</a></li>
      <li><a href="#Part 2">2. Speech Generation Results on LibriSpeech-PC test-clean</a></li>
      <li><a href="#Part 3">3. Speech Generation Results on SEED test-en</a></li>
      <li><a href="#Part 4">4. Results on Long-context Speech Generation</a></li>
      <li><a href="#Part 5">5. More Samples</a></li>
    </ul>
  </div>


  <!-- part 0 -->
  <div class="container">
    <h3 id="Part 0" align="center">Preliminaries: Overall Pipeline of Codec-based TTS and MNCE Loss</h3>
    <div align="center">
      <img src="fig/pipeline.png" width="50%" alt="Figure: overall_pipeline">
      <p class="centered-paragraph" style="width: 75%; margin: 0 auto; text-align: center;">
        Figure: Overall Pipeline of Codec-based TTS.
      </p>
    </div>
    <br>

    <p class="centered-paragraph" style="width: 75%;"> As shown in the figure above, we follow the VALL-E-style
      zero-shot TTS setup during training and inference. This work focuses on improving the CE loss function rather than
      modifying the model
      architecture of any existing TTS system.
    </p>
    <br>

    <p class="centered-paragraph" style="width: 75%;">To help non-TTS readers quickly grasp the framework, we illustrate
      the general codec-based TTS pipeline:
      <br>
      1. When using <strong>WavTokenizer</strong> or <strong>X-codec2</strong> as the speech tokenizer, the TTS task
      requires a language model (LM) to autoregressively generate speech tokens from text and a speech prompt, which
      are then directly reconstructed into audio.<br>
      2. In contrast, when using the <strong>S³ tokenizer</strong>, the pipeline relies on additional components: a
      speaker extractor to derive speaker embeddings from the speech prompt, a flow matching module, and a mel-based
      vocoder, as described in CosyVoice2.
    </p>
    <br>

    <p class="centered-paragraph" style="width: 75%;"> Although these two type of codec-based TTS architectures differ
      in their decoding paths, both use the standard CE loss during LM training. This is exactly where our proposed MNCE
      loss makes targeted improvements to better leverage the neighborhood structure of speech tokens.
    </p>
    <br>

    <div align="center">
      <img src="fig/MNCE_overview.png" width="75%" alt="Figure: overall_mnce_loss">
      <p class="centered-paragraph" style="width: 75%; margin: 0 auto; text-align: center;">
        Figure: Comparison between CE loss and MNCE loss.
      </p>
    </div>
    <br>
    <p class="centered-paragraph" style="width: 75%;"> CE designates the single ground-truth token as a solitary
      point-like positive target and imposes uniform penalties on all non-target tokens, including acoustically similar
      neighboring tokens, completely ignoring the geometric structure of the token space.

      MNCE, by contrast, leverages the geometric similarity of token embeddings to integrate the ground-truth token and
      its neighboring tokens into a regional positive set, only imposing penalties on irrelevant tokens outside this set
      and enabling the model to treat acoustically similar neighboring tokens as partially correct predictions.
    </p>
    <br>

  </div>

  <!-- 2. Part 1: 分析部分 -->
  <div class="container">
    <h3 id="Part 1" align="center">1. Analysis of Neighboring Token Phenomenon in Codec-based TTS</h3>
    <div align="center">
      <img src="fig/reconstruction-wer_sim.png" width="75%" alt="Figure 1">
      <p class="centered-paragraph" style="width: 75%;">Figure 1: Impact of Neighbor Substitution with Probability
        <i>r</i> = 0.25 on the Speech Reconstruction Task. <i>K</i> = 0 indicates that the original token is kept
        unchanged. As <i>K</i> increases, the original token can be replaced by more distant neighbors, which moderately
        increases WER while keeping SIM changes within a narrow range.
      </p>

      <img src="fig/generation-wer_sim.png" width="75%" alt="Figure 2">
      <p class="centered-paragraph" style="width: 75%;">Figure 2: Impact of Neighbor Substitution with Probability
        <i>r</i> = 0.25 on the Speech Generation Task. Across codec-based TTS systems, substituting tokens with their
        neighbors leads to small changes in WER and SIM, suggesting that neighboring tokens are perceptually plausible
        alternatives, validating the neighboring token phenomenon.
      </p>
    </div>
    <br>

    <p class="centered-paragraph" style="width: 75%;"> From above 2 figures, we can draw the following conclusions: <br>
    </p>
    <br>
    <p class="centered-paragraph" style="width: 75%;"> <strong>Speech Reconstruction</strong>. Figure 1 indicates that
      codec
      reconstruction is only mildly affected by neighbor substitution under a token substitution probability of <i>r</i>
      = 0.25. For both X-codec2 and <i>S</i><sup>3</sup> tokenizer, WER gradually increases with minor fluctuations as
      <i>K</i> grows, since more distant neighbors can replace the original tokens and slightly degrade audio quality.
      SIM follows a similar trend: a slight decrease at small <i>K</i>, a modest recovery, and then a slow drift towards
      a stable and slightly worse SIM. Overall, enlarging the neighbor set leads to progressive but small degradations
      in WER and SIM, suggesting that neighboring tokens are perceptually valid substitutes for reconstruction.
    </p>
    <br>
    <p class="centered-paragraph" style="width: 75%;"> <strong>Speech Generation</strong>. As shown in Figure 2, speech
      generation exhibits similarly small changes with increasing <i>K</i> under the substitution probability <i>r</i> =
      0.25. For both Llasa and CosyVoice2, WER and SIM vary only slightly up to <i>K</i> = 10, supporting the conclusion
      that neighboring tokens are acoustically plausible alternatives for language-model-predicted sequences.
      Interestingly, the degradation in speech generation is smaller than that in speech reconstruction. For example,
      with <i>S</i><sup>3</sup> tokenizer, increasing <i>K</i> from 0 to 10 raises reconstruction WER from 2.58 to 3.20
      (absolute +0.62) and decreases SIM from 90.48 to 89.87 (absolute -0.61). In contrast, for speech generation, WER
      increases from 1.69 to 2.05 (absolute +0.36) and SIM decreases from 73.81 to 73.57 (absolute -0.24). This may be
      due to higher tolerance in generation, since swapping a token to an acoustically similar neighbor introduces only
      a small change and the inference stage already uses stochastic sampling that is robust to such variations. </p>
    <br>
    <p class="centered-paragraph" style="width: 75%;">These results confirm: (1) neighboring token
      phenomenon exists not only in the codec but also in the neural codec language model trained upon it, spanning the
      entire codec-based TTS system; (2) tokens can be substituted by near neighbors with negligible perceptual impact.
      These observations motivate our new training objective that accounts for neighboring tokens, rather than treating
      all non-target tokens as equally incorrect. </p>
  </div>
  </div>

  <!-- 3. Part 2: 动态表格部分 -->
  <div class="container">
    <h3 id="Part 2" align="center">2. Speech Generation Results on LibriSpeech-PC test-clean</h3>
    <p class="centered-paragraph">
      We present speech generation samples from diverse codec-based TTS systems using WavTokenizer, X-codec2, and the
      <i>S</i><sup>3</sup> tokenizer. For each setup, we train two codec language models with the same architecture: one
      with the standard cross-entropy (CE) loss, and one with the proposed Multi-Neighbor Cross-Entropy (MNCE) loss, and
      provide side-by-side comparisons. The superscript \( \text{MNCE} \) indicates the neural codec language model
      trained with \( \text{MNCE} \)
      loss, and the subscripts, such as \( \text{960h} \), denote the training data scale.
    </p>

    <!-- 动态加载表格的容器 -->
    <div class="table-container" id="librispeech-table-container">
      <p>Loading table data, please wait...</p>
    </div>

    <!-- 调试信息输出（可选） -->
    <div id="load-text-debug" class="centered-paragraph"
      style="white-space: pre-wrap; font-size: 12px; color: #666; margin-top: 40px;"></div>
  </div>

  <!-- JavaScript 放置在 Body 结束前 -->
  <script>
    async function initLibriSpeech() {
      const display_num_examples = 4;
      const testset = "LibriSpeech";
      const tableContainer = document.getElementById('librispeech-table-container');

      try {
        // --- 1. 获取并解析 test.txt ---
        const testRes = await fetch(`audio/${testset}/test.txt`);
        if (!testRes.ok) throw new Error(`Could not find audio/${testset}/test.txt`);
        const testRaw = await testRes.text();
        const test_infos = testRaw.trim().split('\n').slice(0, display_num_examples).map(line => {
          const parts = line.split('\t');
          return { speech_id: parts[0], gt_text: parts[1], prompt_speech_id: parts[2], prompt_text: parts[3] };
        });

        // --- 2. 获取并解析 model_names.txt (新增映射逻辑) ---
        const nameRes = await fetch(`audio/${testset}/model_names.txt`);
        let nameMap = {};
        if (nameRes.ok) {
          const nameRaw = await nameRes.text();
          nameRaw.trim().split('\n').forEach(line => {
            const firstColonIndex = line.indexOf(':');
            if (firstColonIndex !== -1) {
              const key = line.substring(0, firstColonIndex).trim();
              const value = line.substring(firstColonIndex + 1).trim();
              nameMap[key] = value;
            }
          });
        }

        // 定义一个映射辅助函数
        const getDisplayName = (id) => nameMap[id] || id;

        // --- 3. 获取并解析 models.txt ---
        const modelRes = await fetch(`audio/${testset}/models.txt`);
        if (!modelRes.ok) throw new Error(`Could not find audio/${testset}/models.txt`);
        const modelRaw = await modelRes.text();
        const model_pairs = modelRaw.trim().split('\n').map(line => {
          const parts = line.split(':');
          return {
            baseline: parts[0] ? parts[0].trim() : "",
            variant: parts[1] ? parts[1].trim() : "None"
          };
        });

        // --- 4. 构建表格 ---
        let html = `<table align="center"><thead>`;
        html += `
            <tr>
              <th rowspan="2" class="sticky-column" style="min-width: 110px;">Neural Audio Codec</th>
              <th rowspan="2" style="min-width: 150px;">Neural Codec Language Model</th>
              <th colspan="${display_num_examples}">Samples</th>
            </tr>
            <tr>
              ${test_infos.map((_, i) => `<th style="min-width: 284px;">sample ${i + 1}</th>`).join('')}
            </tr>
          </thead><tbody>`;

        // Ground Truth & Reference 行 (逻辑保持不变)
        html += `
          <tr>
            <td rowspan="2" class="sticky-column">Ground Truth</td>
            <td>Text</td>
            ${test_infos.map(info => `<td>${info.gt_text}</td>`).join('')}
          </tr>
          <tr>
            <td class="empty-cell"></td>
            <td>Target Audio</td>
            ${test_infos.map(info => `
              <td style="text-align: center">
                <audio controls="controls"><source src="audio/${testset}/ground_truth/${info.speech_id}.wav" /></audio>
              </td>`).join('')}
          </tr>
          <tr>
            <td rowspan="2" class="sticky-column">Reference</td>
            <td>Reference Text</td>
            ${test_infos.map(info => `<td>${info.prompt_text}</td>`).join('')}
          </tr>
          <tr>
            <td class="empty-cell"></td>
            <td>Reference Audio</td>
            ${test_infos.map(info => `
              <td style="text-align: center">
                <audio controls="controls"><source src="audio/${testset}/reference/${info.speech_id}.wav" /></audio>
              </td>`).join('')}
          </tr>`;

        // --- 5. 模型对比行 (应用映射) ---
        model_pairs.forEach(pair => {
          if (!pair.baseline) return;
          const hasV = pair.variant !== "None";

          // 处理 Baseline
          const b_parts = pair.baseline.split('+');
          const b_codec_raw = b_parts[0];
          const b_model_raw = b_parts.length > 1 ? b_parts[1] : b_parts[0];

          // 使用映射后的名称
          const b_codec_display = getDisplayName(b_codec_raw);
          const b_model_display = getDisplayName(b_model_raw);

          // Baseline 行渲染
          html += `
            <tr>
              <td rowspan="${hasV ? 2 : 1}" class="sticky-column">${b_codec_display}</td>
              <td>${b_model_display}</td>
              ${test_infos.map(info => `
                <td style="text-align: center">
                  <audio controls="controls"><source src="audio/${testset}/${pair.baseline}/${info.speech_id}.wav" /></audio>
                </td>`).join('')}
            </tr>`;

          // Variant 行渲染
          if (hasV) {
            const v_parts = pair.variant.split('+');
            const v_model_raw = v_parts.length > 1 ? v_parts[1] : v_parts[0];
            const v_model_display = getDisplayName(v_model_raw);

            html += `
              <tr>
                <td class="empty-cell"></td>
                <td>${v_model_display}</td>
                ${test_infos.map(info => `
                  <td style="text-align: center">
                    <audio controls="controls"><source src="audio/${testset}/${pair.variant}/${info.speech_id}.wav" /></audio>
                  </td>`).join('')}
              </tr>`;
          }
        });

        html += `</tbody></table>`;
        tableContainer.innerHTML = html;

        // 渲染 MathJax
        if (window.MathJax && window.MathJax.typeset) {
          window.MathJax.typeset();
        }

      } catch (err) {
        console.error(err);
        tableContainer.innerHTML = `<p style="color:red; font-weight:bold;">Error: ${err.message}</p>`;
      }
    }

    document.addEventListener('DOMContentLoaded', initLibriSpeech);
  </script>


  <div class="container">
    <h3 id="Part 3" align="center">3. Speech Generation Results on SEED test-en</h3>
    <p class="centered-paragraph">
      We present speech generation samples from open-source TTS systems, including variants trained with the
      proposed MNCE (Multi-Neighbor Cross-Entropy) loss. Our reproduced CosyVoice2 system uses the <i>S</i><sup>3</sup>
      tokenizer and a
      Qwen2.5-0.5B-based neural codec language model. The superscript \( \text{MNCE} \) indicates the neural codec
      language model
      trained with \( \text{MNCE} \) loss, and the subscripts, such as \( \text{20Kh} \),
      denote the training data scale.
    </p>

    <!-- 动态加载表格的容器 -->
    <div class="table-container" id="seed-table-container">
      <p>Loading table data, please wait...</p>
    </div>

    <!-- 调试信息输出（可选） -->
    <div id="load-text-debug" class="centered-paragraph"
      style="white-space: pre-wrap; font-size: 12px; color: #666; margin-top: 40px;"></div>
  </div>

  <!-- JavaScript 放置在 Body 结束前 -->
  <script>
    async function initSEED() {
      const display_num_examples = 6;
      const testset = "SEED_test_en";
      const tableContainer = document.getElementById('seed-table-container');

      try {
        // --- 1. 获取并解析 test.txt ---
        const testRes = await fetch(`audio/${testset}/test.txt`);
        if (!testRes.ok) throw new Error(`Could not find audio/${testset}/test.txt`);
        const testRaw = await testRes.text();
        const test_infos = testRaw.trim().split('\n').slice(0, display_num_examples).map(line => {
          const parts = line.split('\t');
          return { speech_id: parts[0], gt_text: parts[1], prompt_speech_id: parts[2], prompt_text: parts[3] };
        });

        // --- 2. 获取并解析 model_names.txt (新增映射逻辑) ---
        const nameRes = await fetch(`audio/${testset}/model_names.txt`);
        let nameMap = {};
        if (nameRes.ok) {
          const nameRaw = await nameRes.text();
          nameRaw.trim().split('\n').forEach(line => {
            const firstColonIndex = line.indexOf(':');
            if (firstColonIndex !== -1) {
              const key = line.substring(0, firstColonIndex).trim();
              const value = line.substring(firstColonIndex + 1).trim();
              nameMap[key] = value;
            }
          });
        }

        // 定义一个映射辅助函数
        const getDisplayName = (id) => nameMap[id] || id;

        // --- 3. 获取并解析 models.txt ---
        const modelRes = await fetch(`audio/${testset}/models.txt`);
        if (!modelRes.ok) throw new Error(`Could not find audio/${testset}/models.txt`);
        const modelRaw = await modelRes.text();
        const model_pairs = modelRaw.trim().split('\n').map(line => {
          const parts = line.split(':');
          return {
            baseline: parts[0] ? parts[0].trim() : "",
            variant: parts[1] ? parts[1].trim() : "None"
          };
        });

        // --- 4. 构建表格 ---
        let html = `<table align="center"><thead>`;
        html += `
          <tr>
            <th rowspan="2" class="sticky-column" style="min-width: 110px;">Neural Audio Codec</th>
            <th rowspan="2" style="min-width: 150px;">Neural Codec Language Model</th>
            <th colspan="${display_num_examples}">Samples</th>
          </tr>
          <tr>
            ${test_infos.map((_, i) => `<th style="min-width: 284px;">sample ${i + 1}</th>`).join('')}
          </tr>
        </thead><tbody>`;

        // Ground Truth & Reference 行 (逻辑保持不变)
        // Ground Truth & Reference 行
        html += `
        <tr>
          <!-- 将 rowspan 改为 1 -->
          <td rowspan="1" class="sticky-column">Ground Truth</td>
          <td>Text</td>
          ${test_infos.map(info => `<td>${info.gt_text}</td>`).join('')}
        </tr>
        <!-- 这里删除了原本的 Target Audio <tr> 块 -->

        <tr>
          <td rowspan="2" class="sticky-column">Reference</td>
          <td>Reference Text</td>
          ${test_infos.map(info => `<td>${info.prompt_text}</td>`).join('')}
        </tr>
        <tr>
          <td class="empty-cell"></td>
          <td>Reference Audio</td>
          ${test_infos.map(info => `
            <td style="text-align: center">
              <audio controls="controls"><source src="audio/${testset}/reference/${info.speech_id}.wav" /></audio>
            </td>`).join('')}
        </tr>`;


        // --- 5. 模型对比行 (应用映射) ---
        model_pairs.forEach(pair => {
          if (!pair.baseline) return;
          const hasV = pair.variant !== "None";

          // 处理 Baseline
          const b_parts = pair.baseline.split('+');
          const b_codec_raw = b_parts[0];
          const b_model_raw = b_parts.length > 1 ? b_parts[1] : b_parts[0];

          // 使用映射后的名称
          const b_codec_display = getDisplayName(b_codec_raw);
          const b_model_display = getDisplayName(b_model_raw);

          // Baseline 行渲染
          html += `
          <tr>
            <td rowspan="${hasV ? 2 : 1}" class="sticky-column">${b_codec_display}</td>
            <td>${b_model_display}</td>
            ${test_infos.map(info => `
              <td style="text-align: center">
                <audio controls="controls"><source src="audio/${testset}/${pair.baseline}/${info.speech_id}.wav" /></audio>
              </td>`).join('')}
          </tr>`;

          // Variant 行渲染
          if (hasV) {
            const v_parts = pair.variant.split('+');
            const v_model_raw = v_parts.length > 1 ? v_parts[1] : v_parts[0];
            const v_model_display = getDisplayName(v_model_raw);

            html += `
            <tr>
              <td class="empty-cell"></td>
              <td>${v_model_display}</td>
              ${test_infos.map(info => `
                <td style="text-align: center">
                  <audio controls="controls"><source src="audio/${testset}/${pair.variant}/${info.speech_id}.wav" /></audio>
                </td>`).join('')}
            </tr>`;
          }
        });

        html += `</tbody></table>`;
        tableContainer.innerHTML = html;

        // 渲染 MathJax
        if (window.MathJax && window.MathJax.typeset) {
          window.MathJax.typeset();
        }

      } catch (err) {
        console.error(err);
        tableContainer.innerHTML = `<p style="color:red; font-weight:bold;">Error: ${err.message}</p>`;
      }
    }

    document.addEventListener('DOMContentLoaded', initSEED);
  </script>




  <div class="container">
    <h3 id="Part 4" align="center">4. Results on Long-context Speech Generation</h3>
    <p class="centered-paragraph">
      We test long-speech generation on LibriSpeech-Long, and the results are shown in the below table. The superscript
      \( \text{MNCE} \) indicates the neural codec language model
      trained with \( \text{MNCE} \)
      loss, and the subscripts, such as \( \text{960h} \), denote the training data scale.
    </p>

    <!-- 动态加载表格的容器 -->
    <div class="table-container" id="librispeechlong-table-container">
      <p>Loading table data, please wait...</p>
    </div>

    <!-- 调试信息输出（可选） -->
    <div id="load-text-debug" class="centered-paragraph"
      style="white-space: pre-wrap; font-size: 12px; color: #666; margin-top: 40px;"></div>
  </div>

  <!-- JavaScript 放置在 Body 结束前 -->
  <script>
    // --- 新增：解析 cer.txt 的辅助函数 ---
    async function fetchCerData(testset, modelId) {
      if (!modelId || modelId === "None") return {};
      try {
        const res = await fetch(`audio/${testset}/${modelId}/eval/cer.txt`);
        if (!res.ok) return {};
        const text = await res.text();
        const results = {};

        // 使用正则匹配每一个数据块
        // 匹配 ID、CER百分比 和 hyp 文本
        // [^]+? 表示非贪婪匹配换行符在内的所有字符
        const blocks = text.split(/(?=^[\w\d_]+\()/m);

        blocks.forEach(block => {
          const idMatch = block.match(/^([\w\d_]+)\(/m);
          const cerMatch = block.match(/cer:([\d.]+)%/);
          const hypMatch = block.match(/^hyp:\s+(.*)$/m);

          if (idMatch && cerMatch && hypMatch) {
            results[idMatch[1]] = {
              cer: cerMatch[1],
              hyp: hypMatch[1].trim()
            };
          }
        });
        return results;
      } catch (e) {
        console.warn(`Failed to fetch CER for ${modelId}:`, e);
        return {};
      }
    }

    async function initLibriSpeechLong() {
      const display_num_examples = 4;
      const testset = "LibriSpeech-Long";
      const tableContainer = document.getElementById('librispeechlong-table-container');

      try {
        // --- 1. 获取并解析 test.txt ---
        const testRes = await fetch(`audio/${testset}/test.txt`);
        if (!testRes.ok) throw new Error(`Could not find audio/${testset}/test.txt`);
        const testRaw = await testRes.text();
        const test_infos = testRaw.trim().split('\n').slice(0, display_num_examples).map(line => {
          const parts = line.split('\t');
          return { speech_id: parts[0], gt_text: parts[1], prompt_speech_id: parts[2], prompt_text: parts[3] };
        });

        // --- 2. 获取并解析 model_names.txt ---
        const nameRes = await fetch(`audio/${testset}/model_names.txt`);
        let nameMap = {};
        if (nameRes.ok) {
          const nameRaw = await nameRes.text();
          nameRaw.trim().split('\n').forEach(line => {
            const firstColonIndex = line.indexOf(':');
            if (firstColonIndex !== -1) {
              const key = line.substring(0, firstColonIndex).trim();
              const value = line.substring(firstColonIndex + 1).trim();
              nameMap[key] = value;
            }
          });
        }
        const getDisplayName = (id) => nameMap[id] || id;

        // --- 3. 获取并解析 models.txt ---
        const modelRes = await fetch(`audio/${testset}/models.txt`);
        if (!modelRes.ok) throw new Error(`Could not find audio/${testset}/models.txt`);
        const modelRaw = await modelRes.text();
        const model_pairs = modelRaw.trim().split('\n').map(line => {
          const parts = line.split(':');
          return {
            baseline: parts[0] ? parts[0].trim() : "",
            variant: parts[1] ? parts[1].trim() : "None"
          };
        });

        // --- 4. 构建表格头部 ---
        let html = `<table align="center"><thead>`;
        html += `
          <tr>
            <th rowspan="2" class="sticky-column" style="min-width: 110px;">Neural Audio Codec</th>
            <th rowspan="2" style="min-width: 150px;">Neural Codec Language Model</th>
            <th colspan="${display_num_examples}">Samples</th>
          </tr>
          <tr>
            ${test_infos.map((_, i) => `<th style="min-width: 284px;">sample ${i + 1}</th>`).join('')}
          </tr>
        </thead><tbody>`;

        // Ground Truth & Reference 行 (保持不变)
        html += `
        <tr>
          <td rowspan="2" class="sticky-column">Ground Truth</td>
          <td>Text</td>
          ${test_infos.map(info => `<td>${info.gt_text}</td>`).join('')}
        </tr>
        <tr>
          <td class="empty-cell"></td>
          <td>Target Audio</td>
          ${test_infos.map(info => `
            <td style="text-align: center">
              <audio controls="controls"><source src="audio/${testset}/ground_truth/${info.speech_id}.flac" /></audio>
            </td>`).join('')}
        </tr>
        <tr>
          <td rowspan="2" class="sticky-column">Reference</td>
          <td>Reference Text</td>
          ${test_infos.map(info => `<td>${info.prompt_text}</td>`).join('')}
        </tr>
        <tr>
          <td class="empty-cell"></td>
          <td>Reference Audio</td>
          ${test_infos.map(info => `
            <td style="text-align: center">
              <audio controls="controls"><source src="audio/${testset}/reference/${info.speech_id}.flac" /></audio>
            </td>`).join('')}
        </tr>`;

        // --- 5. 模型对比行 (加入 CER/WER 数据提取) ---
        for (const pair of model_pairs) {
          if (!pair.baseline) continue;

          // 异步并行获取 Baseline 和 Variant 的 CER 数据
          const [baselineCerMap, variantCerMap] = await Promise.all([
            fetchCerData(testset, pair.baseline),
            fetchCerData(testset, pair.variant)
          ]);

          const hasV = pair.variant !== "None";
          const b_parts = pair.baseline.split('+');
          const b_codec_display = getDisplayName(b_parts[0]);
          const b_model_display = getDisplayName(b_parts.length > 1 ? b_parts[1] : b_parts[0]);

          // Baseline 行
          html += `
          <tr>
            <td rowspan="${hasV ? 2 : 1}" class="sticky-column">${b_codec_display}</td>
            <td>${b_model_display}</td>
            ${test_infos.map(info => {
            const data = baselineCerMap[info.speech_id] || { cer: "N/A", hyp: "N/A" };
            return `
              <td style="text-align: center">
                <audio controls="controls"><source src="audio/${testset}/${pair.baseline}/wav/${info.speech_id}.wav" /></audio>
                <div style="text-align: left; font-size: 0.85em; margin-top: 8px; padding: 0 10px;">
                  <strong>WER:</strong> ${data.cer}% <br>
                  <strong>Transcript:</strong> <span style="color: #555; font-style: italic;">${data.hyp}</span>
                </div>
              </td>`;
          }).join('')}
          </tr>`;

          // Variant 行
          if (hasV) {
            const v_parts = pair.variant.split('+');
            const v_model_display = getDisplayName(v_parts.length > 1 ? v_parts[1] : v_parts[0]);

            html += `
            <tr>
              <td class="empty-cell"></td>
              <td>${v_model_display}</td>
              ${test_infos.map(info => {
              const data = variantCerMap[info.speech_id] || { cer: "N/A", hyp: "N/A" };
              return `
                <td style="text-align: center">
                  <audio controls="controls"><source src="audio/${testset}/${pair.variant}/wav/${info.speech_id}.wav" /></audio>
                  <div style="text-align: left; font-size: 0.85em; margin-top: 8px; padding: 0 10px;">
                    <strong>WER:</strong> ${data.cer}% <br>
                    <strong>Transcript:</strong> <span style="color: #555; font-style: italic;">${data.hyp}</span>
                  </div>
                </td>`;
            }).join('')}
            </tr>`;
          }
        }

        html += `</tbody></table>`;
        tableContainer.innerHTML = html;

        if (window.MathJax && window.MathJax.typeset) {
          window.MathJax.typeset();
        }

      } catch (err) {
        console.error(err);
        tableContainer.innerHTML = `<p style="color:red; font-weight:bold;">Error: ${err.message}</p>`;
      }
    }

    document.addEventListener('DOMContentLoaded', initLibriSpeechLong);
  </script>




  <div class="container">
    <h3 id="Part 5" align="center">5. More Samples</h3>
    <p class="centered-paragraph">
      We present additional samples from the LibriSpeech test-clean set to illustrate how MNCE improves upon the
      baseline CE results. The superscript \( \text{MNCE} \) indicates the neural codec language model
      trained with \( \text{MNCE} \) loss, and the subscripts, such as \( \text{20Kh} \), denote the training data
      scale.
    </p>

    <!-- 动态加载表格的容器 -->
    <div class="table-container" id="compared_examples-table-container">
      <p>Loading table data, please wait...</p>
    </div>
  </div>

  <script>
    /**
     * 辅助函数：解析 JSONL 文件并返回对象数组
     * @param {string} url - JSONL 文件的路径
     */
    async function fetchJsonlData(url) {
      try {
        const res = await fetch(url);
        if (!res.ok) return [];
        const text = await res.text();
        // 将每一行字符串解析为 JSON 对象，过滤掉空行
        return text.trim().split('\n')
          .filter(line => line.trim())
          .map(line => JSON.parse(line));
      } catch (e) {
        console.error("Error loading JSONL:", url, e);
        return [];
      }
    }

    /**
     * 主初始化函数：根据模型对生成对比表格
     */
    async function initComparedExamples() {
      const display_num_examples = 6; // 每行展示的样本数
      const testset = "more_examples"; // 根目录
      const tableContainer = document.getElementById('compared_examples-table-container');

      try {
        // --- 1. 加载基础配置 ---
        const [nameRes, modelRes] = await Promise.all([
          fetch(`audio/${testset}/model_names.txt`),
          fetch(`audio/${testset}/models.txt`)
        ]);

        if (!modelRes.ok) throw new Error("models.txt missing.");

        // 解析模型名称映射
        let nameMap = {};
        if (nameRes.ok) {
          const nameRaw = await nameRes.text();
          nameRaw.trim().split('\n').forEach(line => {
            const idx = line.indexOf(':');
            if (idx !== -1) nameMap[line.substring(0, idx).trim()] = line.substring(idx + 1).trim();
          });
        }
        const getDisplayName = (id) => nameMap[id] || id;

        // 解析模型对比对 (Baseline : Variant)
        const modelRaw = await modelRes.text();
        const model_pairs = modelRaw.trim().split('\n').map(line => {
          const parts = line.split(':');
          return {
            baseline: parts[0] ? parts[0].trim() : "",
            variant: parts[1] ? parts[1].trim() : "None"
          };
        });

        // --- 2. 遍历模型对并构建 HTML ---
        let allTablesHtml = "";

        for (const pair of model_pairs) {
          if (!pair.baseline) continue;

          // 加载 Baseline 的评估数据作为主数据源
          const baselineDataList = await fetchJsonlData(`audio/${testset}/${pair.baseline}/eval/eval.jsonl`);
          if (baselineDataList.length === 0) {
            console.warn(`No data found for baseline: ${pair.baseline}`);
            continue;
          }

          // 取前 display_num_examples 个样本
          const currentSamples = baselineDataList.slice(0, display_num_examples);

          // 如果有 Variant，加载其评估数据并建立 ID 索引映射
          let variantDataMap = {};
          if (pair.variant !== "None") {
            const vList = await fetchJsonlData(`audio/${testset}/${pair.variant}/eval/eval.jsonl`);
            vList.forEach(obj => { variantDataMap[obj.speech_id] = obj; });
          }

          // --- 3. 开始构建当前 Pair 的 HTML 内容 ---

          // 先计算名称用于标题
          const b_parts = pair.baseline.split('+');
          const b_codec_display = getDisplayName(b_parts[0]);
          const b_model_display = getDisplayName(b_parts.length > 1 ? b_parts[1] : b_parts[0]);

          // 生成对比组标题 (英语)
          let html = `<h4 align="center" style="margin-top: 50px; color: #333;">Comparison results in TTS system consisting of ${b_codec_display} and ${b_model_display}</h4>`;

          // 生成表格
          html += `<table align="center" style="margin-bottom: 60px;"><thead>`;
          html += `
              <tr>
                  <th rowspan="2" class="sticky-column" style="min-width: 110px;">Neural Audio Codec</th>
                  <th rowspan="2" style="min-width: 150px;">Neural Codec Language Model</th>
                  <th colspan="${currentSamples.length}">Samples</th>
              </tr>
              <tr>
                  ${currentSamples.map((_, i) => `<th style="min-width: 284px;">sample ${i + 1}</th>`).join('')}
              </tr></thead><tbody>`;

          // --- Ground Truth 大行 (包含 Text 和 Target Audio) ---
          html += `
              <tr>
                  <td rowspan="2" class="sticky-column">Ground Truth</td>
                  <td>Text</td>
                  ${currentSamples.map(s => `<td>${s.gt_text}</td>`).join('')}
              </tr>
              <tr>
                  <td class="empty-cell"></td>
                  <td>Target Audio</td>
                  ${currentSamples.map(s => `
                      <td style="text-align: center">
                          <audio controls="controls"><source src="audio/${testset}/ground_truth/${s.speech_id}.wav" /></audio>
                      </td>`).join('')}
              </tr>`;

          // --- Reference 大行 (包含 Reference Text 和 Reference Audio) ---
          html += `
              <tr>
                  <td rowspan="2" class="sticky-column">Reference</td>
                  <td>Reference Text</td>
                  ${currentSamples.map(s => `<td>${s.prompt_text}</td>`).join('')}
              </tr>
              <tr>
                  <td class="empty-cell"></td>
                  <td>Reference Audio</td>
                  ${currentSamples.map(s => `
                      <td style="text-align: center">
                          <audio controls="controls"><source src="audio/${testset}/reference/${s.speech_id}.wav" /></audio>
                      </td>`).join('')}
              </tr>`;

          // --- 模型行 (Baseline) ---
          const hasV = pair.variant !== "None";

          html += `
              <tr>
                  <td rowspan="${hasV ? 2 : 1}" class="sticky-column">${b_codec_display}</td>
                  <td>${b_model_display}</td>
                  ${currentSamples.map(s => `
                      <td style="text-align: center">
                          <audio controls="controls"><source src="audio/${testset}/${pair.baseline}/wav/${s.speech_id}.wav" /></audio>
                          <div class="metrics-box">
                              <strong>WER:</strong> ${(s.wer).toFixed(2)}% | 
                              <strong>Sim:</strong> ${(s.sim).toFixed(2) || "N/A"} | 
                              <strong>UTMOS:</strong> ${(s.utmos).toFixed(2) || "N/A"}<br>
                              <strong>Transcript:</strong> <span>${s.gen_text || "N/A"}</span>
                          </div>
                      </td>`).join('')}
              </tr>`;

          // --- 模型行 (Variant) ---
          if (hasV) {
            const v_parts = pair.variant.split('+');
            const v_model_display = getDisplayName(v_parts.length > 1 ? v_parts[1] : v_parts[0]);

            html += `
                  <tr>
                      <td class="empty-cell"></td>
                      <td>${v_model_display}</td>
                      ${currentSamples.map(s => {
              const vs = variantDataMap[s.speech_id] || {};
              return `
                          <td style="text-align: center">
                              <audio controls="controls"><source src="audio/${testset}/${pair.variant}/wav/${s.speech_id}.wav" /></audio>
                              <div class="metrics-box">
                                  <strong>WER:</strong> ${(vs.wer || 0).toFixed(2)}% | 
                                  <strong>Sim:</strong> ${(vs.sim ? vs.sim.toFixed(2) : "N/A")} | 
                                  <strong>UTMOS:</strong> ${(vs.utmos ? vs.utmos.toFixed(2) : "N/A")}<br>
                                  <strong>Transcript:</strong> <span>${vs.gen_text || "N/A"}</span>
                              </div>
                          </td>`;
            }).join('')}
                  </tr>`;
          }

          html += `</tbody></table>`;
          allTablesHtml += html;
        }

        // --- 4. 渲染到页面并处理 MathJax ---
        tableContainer.innerHTML = allTablesHtml;

        // 注入指标框的样式
        if (!document.getElementById('metrics-style')) {
          const style = document.createElement('style');
          style.id = 'metrics-style';
          style.innerHTML = `
                  .metrics-box {
                      text-align: left; 
                      font-size: 0.82em; 
                      margin-top: 10px; 
                      background: #fdfdfd; 
                      padding: 8px; 
                      border: 1px solid #eee;
                      border-radius: 4px;
                      line-height: 1.5;
                      color: #333;
                  }
                  .metrics-box span { color: #666; font-style: italic; }
              `;
          document.head.appendChild(style);
        }

        // 重新渲染 LaTeX 公式
        if (window.MathJax && window.MathJax.typeset) {
          window.MathJax.typeset();
        }

      } catch (err) {
        console.error(err);
        tableContainer.innerHTML = `<p style="color:red; font-weight:bold;">Error: ${err.message}</p>`;
      }
    }

    // 页面加载完成后启动
    document.addEventListener('DOMContentLoaded', initComparedExamples);
  </script>




</body>

</html>
